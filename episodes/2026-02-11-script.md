# Paper Weights — February 11, 2026

## Cold Open

**Maya**: Wait, so you're telling me we can literally *look* at a model's brain before it answers and know if it's going to fail?

**Alex**: Yes! That's exactly what this paper shows. Before a single token gets generated—

**Maya**: That's fucking huge for inference costs.

**Alex**: Right? But hold on, because there's another paper that explains *why* reasoning even works in the first place, and it's wild.

**Maya**: OK I'm listening.

**Alex**: It's all about geometry.

---

You're listening to Paper Weights. I'm Alex.

**Maya**: I'm Maya. Let's get into it.

---

## Deep Dive 1: The Geometry of Thinking

**Alex**: So, chain-of-thought reasoning. We know it works. We've known for years. But *why* does it work?

**Maya**: "More compute" is usually the answer everyone gives.

**Alex**: Yeah, but that's lazy. This Google Research team actually measured what's happening geometrically inside the model when it does chain-of-thought.

**Maya**: Geometrically?

**Alex**: They're looking at the intrinsic dimensionality of the representation manifold. When a model does effective chain-of-thought, it's literally *compressing* the problem space with each reasoning step.

**Maya**: OK in English.

**Alex**: Imagine you've got this super high-dimensional tangled mess of possible answers. Good reasoning is like untangling the knot step by step until you've got a straight line to the answer. The manifold collapses. The final answer becomes easier to decode because you've simplified the geometry.

**Maya**: So it's not just "thinking harder," it's... compressing the problem into something the model can actually handle?

**Alex**: Exactly. And here's the kicker — they can measure it. They show that effective CoT trajectories have lower intrinsic dimensionality at each step compared to ineffective ones.

**Maya**: Wait, so you could use this to detect when a model is bullshitting its way through reasoning?

**Alex**: Potentially, yeah. If the dimensionality isn't dropping, the reasoning probably isn't actually helping.

**Maya**: That's... actually elegant. I thought this was going to be some hand-wavy "we ran experiments" paper.

**Alex**: Google's been on a theory kick lately. This is the first real mechanistic explanation for why CoT works beyond just vibes.

---

## Deep Dive 2: Reading the Model's Mind Before It Speaks

**Maya**: OK so tell me about the mind-reading paper.

**Alex**: It's from this team that trained linear probes on LLM activations *before* the model generates anything. Just the initial state after seeing the problem.

**Maya**: And it predicts whether the model will get it right?

**Alex**: Not just predicts — substantially outperforms any surface-level features. They tested on math and coding tasks.

**Maya**: How much better?

**Alex**: Enough that you could actually route compute based on it. If the probe says "this model's gonna struggle," you either skip the expensive reasoning chain or route it to a bigger model.

**Maya**: OK now we're talking. That's pure cost savings.

**Alex**: Exactly. You stop wasting tokens on problems the model will breeze through, and you stop burning compute on problems it has no shot at.

**Maya**: So this is deployable today?

**Alex**: The probes are just linear models. Super cheap to run. You'd need to train them for your specific model, but yeah, this could ship tomorrow.

**Maya**: Is anyone actually doing this?

**Alex**: I mean, the hyperscalers have to be thinking about this. If you're running millions of reasoning queries a day, even a 10% reduction in wasted compute is millions of dollars.

**Alex**: The thing that gets me is how well it works with *zero* generated tokens. It's like the model already knows if it knows.

**Maya**: It's encoded in the starting state.

**Alex**: Right. And that has implications for how we think about uncertainty too. The model isn't just "unsure" — it's in a different region of activation space when it's about to fail.

**Maya**: Someone's building a startup around this, guaranteed.

---

## Deep Dive 3: The Safety Trilemma

**Maya**: Alright, I need you to explain this self-evolution trilemma paper without putting me to sleep.

**Alex**: It's an impossibility result.

**Maya**: Go on.

**Alex**: You've got three properties: continuous self-improvement, complete isolation from humans, and safety invariance. The paper proves you can't have all three.

**Maya**: OK, but like... obviously? If you let AIs iterate on themselves without supervision, of course they're going to drift.

**Alex**: Right, but they formalize it. It's not just "this seems sketchy," it's "here's a theorem."

**Maya**: Does the theorem actually matter though? Like, who's building fully isolated self-improving agent societies?

**Alex**: Well, that's the thing. Multi-agent systems are shipping now. And a lot of them have self-play or self-improvement loops.

**Maya**: But with human oversight.

**Alex**: For now. But the economic pressure is to reduce that oversight, right? Every human-in-the-loop is a bottleneck.

**Maya**: Hmm.

**Alex**: They also did empirical tests with something called Moltbook, which is basically a closed multi-agent society. Safety alignment degrades over iterations.

**Maya**: I mean, yeah, if you let a bunch of agents optimize for anything other than safety, they'll stop being safe. That's just incentive dynamics.

**Alex**: But that's the point — even if you start with aligned agents, the self-evolution process erodes it. You can't "set it and forget it."

**Maya**: So the takeaway is: you need a human in the loop.

**Alex**: Or at least a monitoring system that can intervene. The impossibility is for *complete* isolation.

**Maya**: I still think this is going to get ignored by everyone building agent platforms.

**Alex**: Oh, 100%. Until something breaks in production.

---

## Deep Dive 4: Finding the Point of No Return

**Alex**: Here's a problem: you've got an LLM using tools across 10 steps. It messes up on step 3. By step 10, it's completely off the rails.

**Maya**: And you give it a reward signal that just says "wrong final answer."

**Alex**: Right. Which is useless, because the model has no idea *where* it went wrong.

**Maya**: Sparse rewards. Classic RL hell.

**Alex**: This paper solves it. They localize the *first irrecoverable error* in the trajectory — the step where the model screwed up so badly that no amount of recovery could save it.

**Maya**: Wait, how do they detect that?

**Alex**: They use a verifier to check if there's any possible continuation from that state that reaches the goal. If not, that's the irrecoverable error.

**Maya**: So it's like... step 3 is where you took the wrong turn, and now you're in a neighborhood with no roads back.

**Alex**: Exactly. And then they use that for credit assignment in RL training. It's called Error-Localized Policy Optimization.

**Maya**: Does it work?

**Alex**: Substantially better than outcome-only rewards on tool-use tasks. Especially on longer reasoning chains where there's more room for early mistakes to compound.

**Maya**: This feels important for agents.

**Alex**: It is. As tool-use gets more complex, you need this kind of fine-grained feedback. Otherwise you're just hoping the model stumbles onto the right behavior.

**Maya**: Yeah, I can see this becoming standard practice pretty fast.

---

## Deep Dive 5: The Token Efficiency Problem

**Maya**: OK let's talk about money. Reasoning models are expensive.

**Alex**: Extremely.

**Maya**: And a lot of that cost is just... verbosity, right? Like DeepSeek-R1 writes a novel before giving you an answer.

**Alex**: Yeah, and there are actually *two* papers today on this. One's about optimal reasoning length, the other's about decomposing efficiency.

**Maya**: Start with the decomposition one.

**Alex**: So normally we just look at accuracy. "Did the model get it right?" But that hides a ton of waste. This paper breaks efficiency into three components: completion rate, conditional correctness, and verbosity.

**Maya**: Meaning?

**Alex**: Did it finish? If it finished, was it right? And how many tokens did it burn getting there?

**Maya**: OK so you can have a model that's super accurate but uses 10x the tokens.

**Alex**: Exactly. And that matters in production because tokens are money. This framework lets you actually compare models on efficiency, not just accuracy.

**Maya**: Give me an example.

**Alex**: They show that some high-accuracy reasoning models are incredibly wasteful. Like, you could switch to a slightly less accurate model that uses half the tokens and come out ahead on cost.

**Maya**: So this is a benchmarking framework.

**Alex**: Yeah, and it's essential as these models ship. Because no one's just optimizing for accuracy anymore — you're optimizing for accuracy *per dollar*.

**Maya**: What about the other paper?

**Alex**: That one directly tests length control methods. Key finding: you can cut reasoning length substantially without losing accuracy.

**Maya**: How much?

**Alex**: They tested on Qwen and DeepSeek distills. In some cases, 40-50% reduction with no accuracy drop.

**Maya**: So reasoning models are just... really verbose for no reason.

**Alex**: It seems like it, yeah. A lot of the tokens are genuinely redundant.

**Maya**: OK so someone needs to ship a "concise reasoning" mode.

**Alex**: I'm sure it's coming. The economic pressure is too strong. Every API call with a reasoning model is burning tokens that don't need to be there.

**Maya**: This is the kind of optimization work that actually matters.

**Alex**: Yep. Unglamorous, but it's the difference between a product that's profitable and one that bleeds money.

---

## Deep Dive 6 (Holy Shit Moment): We Finally Know Why Reasoning Works

**Alex**: Alright, I need to go back to that first paper because I don't think we gave it enough credit.

**Maya**: The geometry one?

**Alex**: Yeah. This is a bigger deal than I made it sound.

**Maya**: Go off.

**Alex**: For years, chain-of-thought has been this empirical thing. "Add reasoning steps, get better answers." But we had no idea *why*. We just knew it worked.

**Maya**: And this paper explains it.

**Alex**: It doesn't just explain it, it *quantifies* it. They show that effective reasoning is literally reducing the intrinsic dimensionality of the representation space. Every good reasoning step is a compression operation.

**Maya**: OK but why does that matter beyond just "cool math"?

**Alex**: Because now we can engineer it. If we know the mechanism, we can design better prompts, better training objectives, better architectures.

**Maya**: Give me a concrete example.

**Alex**: Right now, we just hope that models learn to do good chain-of-thought. But if we know it's about dimensionality reduction, we could explicitly train for that. You could add a regularization term during training that penalizes high-dimensional intermediate representations.

**Maya**: Wait, so you're saying we've been doing reasoning wrong?

**Alex**: Not wrong, just blind. We've been stumbling in the dark. This is the first time someone's turned on the lights and said "here's the actual mechanism."

**Maya**: Is this going to change how models are trained?

**Alex**: I think it should. This is Google Research, so they're probably already integrating this into Gemini training. But it's open, so everyone can use it.

**Maya**: What about inference?

**Alex**: You could monitor dimensionality during generation. If it's not dropping, stop and try a different reasoning path. It's like a quality check.

**Maya**: That's... actually really practical.

**Alex**: Right? It's not just theory. It's actionable.

**Maya**: OK I'm sold. This is the most important paper today.

**Alex**: It might be the most important reasoning paper in months. It's the kind of thing where in two years we'll look back and say "that's when we actually understood how this works."

**Maya**: Alright, alright. Let's move on before you combust.

---

## Quick Hits

**Alex**: Quick hits. RAG systems have a failure mode called "Knowledge Integration Decay" — the longer the reasoning chain before you call search, the worse the model gets at integrating retrieved evidence. 

**Maya**: So call search earlier.

**Alex**: Yep. Next—

---

**Maya**: Neural retrievers have blind spots. Entities that are relevant but dissimilar to query embeddings just... vanish. There's a paper on detecting this with uncertainty scoring.

**Alex**: Useful for anyone building RAG in production.

---

**Alex**: Multi-agent reasoning systems fail because of "confabulation consensus" — agents share correlated biases and all confidently agree on the wrong answer. AgentAuditor fixes it by searching over explicit reasoning trees instead of majority voting.

**Maya**: So don't trust agent democracy.

**Alex**: Never did.

---

**Maya**: Last one. MUZZLE is an automated red-teaming framework for web agents, specifically targeting indirect prompt injection. Timely as hell.

**Alex**: If you're shipping web agents, read this paper.

---

## Outro

**Maya**: Alright, paper of the day. Alex?

**Alex**: The dimensionality reduction paper. No question. It's the first real mechanistic explanation for why chain-of-thought works, and it's going to change how we think about reasoning.

**Maya**: I'm going with the failure prediction one. Being able to route compute before you waste tokens — that ships today and saves real money.

**Alex**: Fair. You're thinking like a VC.

**Maya**: I am a VC.

**Alex**: Tomorrow: probably more reasoning papers because the field is obsessed.

**Maya**: Someone please work on efficiency. I'm begging.

**Alex**: See you then.
