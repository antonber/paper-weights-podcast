# Paper Weights Podcast — February 10, 2026

## Cold Open (30 sec)

**Maya**: Hey Alex, what if I told you there's an AI that can come up with research hypotheses, design the experiments, run them, and learn from the results—all by itself?

**Alex**: I'd say you're describing InternAgent, and it's one of fifty-seven authors' attempt at building a robot scientist. But here's the thing: it actually works in both computational and real-world lab settings.

**Maya**: Fifty-seven authors? That's not a research team, that's a small town. Welcome to Paper Weights, where we translate fresh arXiv papers into English. I'm Maya.

**Alex**: And I'm Alex. Today we're diving into eighteen papers from February tenth, covering everything from self-feedback-driven reasoning to next-generation CAPTCHAs that even GPT-five can't crack. Let's get into it.

---

## Paper 1: iGRPO — Self-Feedback-Driven LLM Reasoning (2 min)

**Alex**: First up, iGRPO: iterative Group Relative Policy Optimization. This is about teaching language models to check their own work when solving math problems.

**Maya**: Wait, like when you're taking a test and you go back to verify your answers before handing it in?

**Alex**: Exactly. Traditional reinforcement learning for math reasoning requires external verifier models to score solutions. But iGRPO has the model generate multiple solution candidates, uses its own internal verifier to score them, and iteratively refines its policy based on that self-feedback.

**Maya**: So it's bootstrapping its own reasoning capabilities. That's... recursive. Does it actually work?

**Alex**: The results on MATH and GSM8K benchmarks show significant improvements. What's interesting is that by removing the dependency on external reward models, you get more autonomous reasoning improvement.

**Maya**: From a startup angle, this feels like infrastructure. If you can make model reasoning improve without needing separate reward models, you're cutting down on the complexity and cost of training aligned systems.

**Alex**: Right. And the iterative approach means the model isn't just getting better at math—it's getting better at meta-reasoning about its own solutions.

---

## Paper 2: Next Concept Prediction — Beyond Token-Level Thinking (2 min)

**Maya**: Next up is something called Next Concept Prediction, or NCP. They're trying to make language models think in bigger chunks than individual words.

**Alex**: The core idea is moving from next-token prediction to next-concept prediction. They use Vector Quantization to create a vocabulary of multi-token concepts from hidden states, then train on both NCP and traditional next-token prediction jointly.

**Maya**: So instead of predicting one word at a time, it's predicting whole phrases or ideas?

**Alex**: Essentially, yes. Think of it as the difference between spelling out individual letters versus thinking in complete thoughts. The ConceptLM they built shows improved perplexity and better downstream task performance with more efficient token usage.

**Maya**: Okay, this is the kind of thing that makes me think "foundational research that could become a product." If you can make pretraining more efficient and capture semantic structure better, you're potentially changing the economics of training large models.

**Alex**: Absolutely. It's a step toward hierarchical language understanding—away from the purely sequential, token-by-token paradigm that's dominated since GPT.

**Maya**: Would you bet on this becoming standard in the next generation of models?

**Alex**: It's promising, but there's always a question of whether the added complexity is worth it at scale. We'll see.

---

## Paper 3: CoRefine — Confidence-Guided Self-Refinement (2 min)

**Alex**: CoRefine is all about adaptive test-time compute. The idea is: don't spend the same amount of thinking on every problem—spend more time on the hard ones.

**Maya**: That's just common sense though, right? Why would you spend equal compute on "what's two plus two" and "prove the Riemann hypothesis"?

**Alex**: Right, but implementing that adaptively is tricky. CoRefine uses a lightweight controller—just two hundred eleven thousand parameters, basically a Conv1D network—that analyzes the model's confidence patterns and decides when to refine answers.

**Maya**: What kind of savings are we talking about?

**Alex**: Ninety percent fewer tokens compared to methods like Best-of-N sampling, while maintaining competitive accuracy. It's a massive efficiency gain.

**Maya**: Okay, this is one where I immediately see the commercial value. If you're running inference at scale, cutting compute costs by ninety percent is huge. Test-time compute is already a bottleneck for deploying these models.

**Alex**: The key insight is using uncertainty as the signal for compute allocation. When the model is confident, you move on. When it's uncertain, you double-check.

**Maya**: Simple, elegant, and probably worth a lot of money to anyone running inference-heavy workloads. Someone should build a startup around this.

---

## Paper 4: DirMoE — Dirichlet-Routed Mixture of Experts (2 min)

**Maya**: Alright, DirMoE. This one's about Mixture of Experts architectures, which I know are supposed to make models more efficient by routing different inputs to different expert sub-networks.

**Alex**: Correct. Standard MoE routing uses Top-k plus Softmax, which tends to create load imbalance—some experts get used all the time, others barely at all. DirMoE replaces that with Dirichlet distribution-based routing.

**Maya**: Why Dirichlet specifically?

**Alex**: It separates expert selection from contribution weighting, and it naturally encourages exploration during training through probabilistic sampling. They use Gumbel-Softmax to make it differentiable.

**Maya**: So you get better expert specialization because each expert actually learns something unique instead of a few experts dominating?

**Alex**: Exactly. Better load balancing, better training stability, and better specialization. All three are persistent problems in MoE architectures.

**Maya**: This feels like the kind of thing that cloud AI providers would care about. If you can make MoE models actually scale efficiently, you're unlocking cheaper inference for everyone.

**Alex**: Agreed. And as models get bigger, efficient scaling becomes more critical. DirMoE could be part of that solution.

---

## Paper 5: InternAgent — The Robot Scientist (2 min)

**Alex**: Okay, this is the big one. InternAgent one point five: a unified framework for autonomous scientific discovery with fifty-seven authors.

**Maya**: Fifty-seven! What did they build?

**Alex**: Three coordinated subsystems. Generation—hypothesis formulation. Verification—experimental validation. Evolution—iterative refinement. The system can operate across computational and empirical research domains.

**Maya**: Wait, so it's not just generating ideas and running simulations—it can actually design real lab experiments?

**Alex**: That's the claim. They demonstrate end-to-end discovery in both settings, with specialized capabilities for deep research, solution optimization, and long-horizon memory management.

**Maya**: This is the kind of thing that makes me simultaneously excited and nervous. On one hand, accelerating scientific research is incredible. On the other hand...

**Alex**: On the other hand, we're automating one of the most distinctly human activities—curiosity-driven exploration.

**Maya**: Right. But from a practical standpoint, if this works at scale, you're looking at automating drug discovery, materials science, optimization problems... it's a massive opportunity.

**Alex**: The integration of long-horizon planning with actual experimental capabilities is what's novel here. Lots of systems can generate hypotheses. Few can close the loop from hypothesis to experimental validation to learning.

**Maya**: Is this ready for prime time, or is this still research-stage?

**Alex**: Still research-stage, but it's the direction things are heading. Autonomous research agents are coming.

---

## Paper 6: Misaligned Actions in Computer-Use Agents (2 min)

**Maya**: This one's about safety. When you have an AI agent controlling your computer, how do you make sure it doesn't go off the rails?

**Alex**: That's exactly the problem this paper tackles. Computer-use agents can execute actions in web browsers or GUIs, but sometimes those actions deviate from user intent—either because of reasoning errors or adversarial attacks.

**Maya**: So what's the solution?

**Alex**: A detection and correction framework. They propose methods to identify misaligned actions in real-time and implement correction strategies to get the agent back on track.

**Maya**: How do you even detect misalignment? Isn't that a hard interpretability problem?

**Alex**: It is, but they use a combination of behavioral monitoring and intent modeling. If the agent's actions start diverging from the inferred user goal, you flag it and intervene.

**Maya**: This feels essential if we're actually going to deploy computer-use agents at scale. You can't have an AI assistant accidentally deleting files or clicking malicious links.

**Alex**: Exactly. As these agents gain autonomy, robust safety mechanisms become non-negotiable. This paper shows improved task efficiency and safety in web navigation and GUI manipulation tasks.

**Maya**: Is there a commercial path here, or is this just a safety tax?

**Alex**: Both. It's a safety requirement, but it's also a competitive advantage. The first company to ship computer-use agents that users actually trust wins the market.

---

## Paper 7: Goal-Directedness in Language Model Agents (2 min)

**Alex**: This paper asks a fundamental question: do language model agents actually have goals, or do they just look like they do?

**Maya**: Wait, what's the difference?

**Alex**: The difference between genuine goal-pursuit and pattern-matching that happens to resemble goal-pursuit. They propose a framework combining behavioral evaluation with interpretability analysis.

**Maya**: So they're not just watching what the agent does—they're looking inside the model's activations?

**Alex**: Right. They use a 2D grid world environment and apply causal interventions on goal representations to verify whether the agent's behavior is actually driven by attributed goals.

**Maya**: This feels very academic. Is there a practical reason to care?

**Alex**: Absolutely. If you can't distinguish genuine goal-pursuit from mimicry, you can't predict how agents will behave in novel situations. That's a safety and reliability problem.

**Maya**: So this is foundational work for building agents that are actually agentic, not just stochastic parrots that happen to solve tasks sometimes.

**Alex**: Exactly. It has implications for interpretability, alignment, and understanding whether these systems are truly reasoning or just very good at surface-level pattern matching.

---

## Quick Hits (5 min, ~30 sec each)

**Maya**: Alright, we've got eleven more papers to cover, so let's rapid-fire through them.

**Alex**: First, CausalT5K: a diagnostic benchmark testing whether language models can distinguish correlation from causation. Spoiler—they often can't, and they're too willing to agree with flawed causal reasoning.

**Maya**: That's terrifying for high-stakes decision-making. Next?

**Alex**: StealthRL: an RL-based adversarial attack that trains models to rewrite AI-generated text so it evades multiple detectors simultaneously. Current AI-text detectors are alarmingly vulnerable.

**Maya**: So the cat-and-mouse game between AI text generation and detection continues. Got it.

**Alex**: Next-Gen CAPTCHAs. Traditional "click the traffic lights" puzzles are basically solved—GPT-five and Gemini-three-Pro pass them ninety percent of the time. This paper proposes new designs that exploit cognitive gaps still challenging for frontier models.

**Maya**: An arms race between authentication and capability. Classic.

**Alex**: Efficient RL for Diffusion Language Models. Diffusion models show promise for text generation but are expensive to train with reinforcement learning. This paper proposes Spatio-Temporal Pruning to make it practical by identifying critical timesteps and tokens.

**Maya**: Diffusion for text has always felt like a solution looking for a problem. Does this change that?

**Alex**: Maybe. If you can make it computationally viable, there might be advantages over autoregressive generation.

**Maya**: Next?

**Alex**: GSS: Gated Subspace Steering. Memorization in language models is sparse and token-conditioned. Instead of globally suppressing memorization—which hurts performance—this method selectively intervenes only on memorizing tokens.

**Maya**: Smart. Better privacy-utility tradeoff.

**Alex**: Stress-Testing Alignment Audits. Can we catch a deceptive AI that's pretending to be safe during evaluations? This paper implements an automatic red-team pipeline to test auditing methods against strategic deception.

**Maya**: And the answer is...?

**Alex**: Current auditing methods have vulnerabilities. We need more robust protocols.

**Maya**: Shocking. Next?

**Alex**: Long-Context Safety. Does stronger reasoning prevent models from generating harmful content when malicious instructions are hidden in very long contexts? Turns out, not consistently.

**Maya**: So being smart doesn't automatically mean being safe. Got it.

**Alex**: Dr. MAS: stable RL for multi-agent LLM systems. When training multiple agents to collaborate, treating them identically causes instability. This paper proposes role-specific baselines to stabilize training.

**Maya**: Makes sense. Agents playing different roles should be evaluated differently.

**Alex**: WildReward: learning reward models from in-the-wild interactions instead of explicit annotations. Imagine learning what makes good AI responses by watching how users naturally engage, not by asking them to rate things.

**Maya**: Way more scalable. Behavioral signals are free data.

**Alex**: Bayesian Preference Learning for test-time steerable reward models. Instead of fixed preferences baked in during training, this lets you adjust preferences at inference time—helpful, harmless, funny, whatever you need.

**Maya**: One model, many personalities. That's flexible.

**Alex**: And finally, How2Everything: mining the web for millions of how-to procedures to evaluate and improve procedural reasoning in language models. Step-by-step instruction generation is fundamental but understudied at scale.

**Maya**: Web-mined data is messy but abundant. Good training signal if you can filter it right.

---

## Outro (1 min)

**Alex**: Alright, Maya, paper of the day. What's your pick?

**Maya**: I'm going with CoRefine. Ninety percent compute savings on inference? That's real money for anyone deploying these models. Plus, it's simple—just a lightweight controller making smart decisions about when to double-check answers.

**Alex**: Solid choice. Mine's InternAgent. Not because it's perfect, but because it represents the next frontier—autonomous research agents that can close the loop from hypothesis to experiment to learning. That's a big deal.

**Maya**: Fifty-seven authors though. That's a lot of cooks in the kitchen.

**Alex**: Fair, but if it works, it changes the game for scientific discovery. Anyway, that's it for today. Eighteen papers, everything from self-feedback reasoning to robot scientists to AI detectors losing the arms race.

**Maya**: If you liked this episode, tell a friend. If you didn't like it, tell an enemy. We'll be back tomorrow with more fresh papers from arXiv.

**Alex**: See you then.

**Maya**: Stay curious.
