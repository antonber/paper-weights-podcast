# Paper Weights — February 13, 2026

## Cold Open

**Maya**: Wait, so Anthropic just released a tool that lets you X-ray any two AI models and see exactly where their brains diverge?

**Alex**: Yeah, and it works across completely different architectures. Like comparing GPT-4 to Claude even though they're built totally differently.

**Maya**: That's— OK that's a big deal. Because right now when a new model drops, we have no idea what changed under the hood.

**Alex**: Exactly. It's like trying to figure out if a self-driving car is safer by just... driving it around and hoping for the best.

**Maya**: This is Paper Weights. I'm Maya.

**Alex**: I'm Alex. Let's get into it.

---

## Deep Dives

**Alex**: So yeah, that Anthropic paper. It's called crosscoders, and the core idea is— you know how normally if you want to compare two models, you need them to have the same architecture? Like comparing a base model to its fine-tuned version?

**Maya**: Right, because otherwise the internals are incompatible. Different hidden dimensions, different layer structures.

**Alex**: This paper throws that constraint out. They train a crosscoder that learns to translate between the internal representations of two totally different models. So you can now diff GPT versus Claude, or Gemini versus Llama— whatever. Unsupervised, automated discovery of where they think differently.

**Maya**: OK so this is like... instead of crash-testing every new model, you can just scan it against a known-safe baseline and flag the differences?

**Alex**: Exactly. Anthropic's mech-interp team built this because model diffing is essential for safety auditing, but it's been useless for cross-architecture comparisons. Now it's not.

**Maya**: So who's the customer here? Is this a product or just research?

**Alex**: Probably stays internal for now. Anthropic's safety tooling. But if they open-source it, every lab will use it. It's the kind of infrastructure that makes the whole ecosystem safer.

**Maya**: Hmm. I'm not convinced this stops a determined bad actor, but yeah, it definitely raises the bar for safety eval. I like it.

---

**Maya**: Next one pisses me off, actually.

**Alex**: Oh?

**Maya**: So all the reasoning model hype— o1, o3, DeepSeek-R1, whatever— it only works on math and code. Because those domains have verifiable answers. You can check if the model got it right.

**Alex**: Right, that's the RLVR paradigm. Reinforcement learning from verifiable rewards.

**Maya**: But like, 90% of real-world reasoning is NOT verifiable. Legal arguments, business strategy, medical diagnosis— you can't just plug in a ground truth and train on it.

**Alex**: OK, so this paper— Native Reasoning Models— they claim they can train reasoning on unverifiable data. No external verifiers, no human-annotated reasoning traces.

**Maya**: Wait, how?

**Alex**: They use a self-consistency check. The model generates multiple reasoning paths for the same prompt, and if they converge to similar conclusions, that's treated as a proxy for correctness. You're basically using the model's own internal agreement as the reward signal.

**Maya**: Hmm. That feels... circular?

**Alex**: It is, but it's not wrong. Like, if you ask me a subjective question and I give you five completely different answers depending on how I phrase my reasoning, that's a red flag. Consistency is a weak signal, but it's better than nothing.

**Maya**: OK but does it actually work at scale? Because this sounds like the kind of thing that works on a toy dataset and then falls apart when you try to apply it to messy real-world prompts.

**Alex**: Fair. The paper's light on empirical validation across domains. But the principle is sound. If this works, it's a huge unlock for reasoning models— you're no longer bottlenecked by domains with automatic grading.

**Maya**: I want to believe it. I'm just... skeptical. Check back in six months and see if anyone's actually deployed this.

---

**Alex**: Alright, MiniCPM-SALA. This one's practical.

**Maya**: Give me the problem first.

**Alex**: Long-context models. Everyone wants 1 million token windows, right? But there's a brutal trade-off. Sparse attention gives you high-fidelity understanding of the full context, but it's memory-hungry. Linear attention is memory-efficient, but it loses details— you get this smoothed-out blurry version of the document.

**Maya**: So you pick your poison.

**Alex**: Until now, yeah. This paper says: why not both? Hybrid architecture. Sparse attention for the parts that matter, linear attention for the filler. They call it SALA— Sparse And Linear Attention.

**Maya**: OK that's actually elegant. Do they have benchmarks?

**Alex**: 9 billion parameters, handles long contexts efficiently without the quality drop you see in pure linear models. The smart part is how they decide which tokens get sparse versus linear treatment— it's learned, not hard-coded.

**Maya**: This feels deployable. Like, you could ship this in a product today.

**Alex**: Exactly. It's not a research toy. It's a production-ready long-context solution at a reasonable model size.

**Maya**: Alright, I'm in. Who's building a company on this?

**Alex**: Probably someone already is.

---

**Maya**: Next one's from the GPT-OSS team. They took their 120 billion parameter reasoning model and shrunk it to 88 billion— 27% smaller— with basically no quality loss.

**Alex**: How?

**Maya**: Mixture-of-experts pruning. Post-training neural architecture search. They figured out which experts are redundant and just... deleted them. Plus some clever tricks like FP8 quantization for the key-value cache.

**Alex**: Wait, so they trained a 120B model and then discovered it didn't need 32 billion of those parameters?

**Maya**: Yup.

**Alex**: That's— OK that's a little embarrassing for the original training run.

**Maya**: Or it's just evidence that MoE models are naturally over-parameterized. Like, you don't know which experts you need until you actually train the thing and see what it learns.

**Alex**: Fair. But still, 27% is a huge efficiency gain. Lower serving costs, faster inference.

**Maya**: Yeah, and this is immediately deployable. It's not theoretical. They did it, it works, you can ship it tomorrow.

**Alex**: Do you think this generalizes? Like, can you take any MoE reasoning model and prune it down by 20-30%?

**Maya**: I bet you can. Which means a lot of labs are burning money running bigger models than they need to.

**Alex**: Ouch.

---

**Alex**: OK, this next one is the holy shit moment.

**Maya**: Go.

**Alex**: It's called the Pensieve Paradigm. StateLM. The idea is— right now, context windows are passive. You shove documents into the model's context, and it reads them. But the model doesn't get to choose what to remember or what to retrieve. That's all done by the application layer, by RAG pipelines that humans engineered.

**Maya**: Right, because the model has no agency over its own memory.

**Alex**: This paper gives the model agency. StateLM learns to autonomously manage its own context. It can extract memories, store them in external storage, and retrieve them later when it needs them. It's like giving the model its own personal RAG system that it controls.

**Maya**: Wait, so instead of me deciding what context is relevant, the model decides?

**Alex**: Exactly. The model learns what to remember and when to look it up. It's not just passively receiving context— it's actively curating it.

**Maya**: OK that's fucking wild. Because right now, context management is this huge pain point. You're constantly tweaking retrieval strategies, reranking documents, tuning chunk sizes— and the model just sits there passively consuming whatever you feed it.

**Alex**: Yeah, and this flips that. The model becomes an active participant in its own memory management. It's like the difference between being read to versus reading a book yourself and taking notes.

**Maya**: So what's the catch? Because this sounds too good.

**Alex**: The catch is training. You need to teach the model when to extract memories and when to retrieve them. That's a non-trivial credit assignment problem. The paper shows it works in controlled settings, but scaling this to general-purpose foundation models— that's an open question.

**Maya**: But if it works— like if this paradigm actually scales— this changes how we think about context windows entirely. Because you're no longer limited by the model's fixed context size. You're limited by the quality of the model's memory retrieval.

**Alex**: Exactly. It's a fundamentally different framing of the problem.

**Maya**: OK I'm calling it— if this scales, some major lab will pivot to StateLM-style training within 18 months.

**Alex**: Bold. I'll take that bet.

---

**Maya**: Next one's practical again. It's about reasoning models wasting tokens.

**Alex**: Oh yeah, the overthinking problem.

**Maya**: Right. So o1 and o3 and DeepSeek-R1— they generate these huge internal reasoning traces. But a lot of it is just... circular. The model asks itself the same question five different ways, reflects on its reflection, goes in loops.

**Alex**: It's like watching someone think out loud and realizing half of it is just noise.

**Maya**: Exactly. And that's a problem because every wasted token costs money and adds latency. This paper— Stop Unnecessary Reflection— they show that as problems get more complex, models waste MORE tokens on unnecessary reflection. Which is the opposite of what you want.

**Alex**: So what's the fix?

**Maya**: Adaptive reflection control. Basically, the model learns when to stop reflecting. For easy problems, short reasoning. For hard problems, long reasoning. And they add a length-coordinated penalty to discourage circular reasoning.

**Alex**: Does it hurt accuracy?

**Maya**: No, that's the key. Same accuracy, way fewer tokens. They tested it on GSM8K and MATH— standard reasoning benchmarks— and got like 30-40% token savings with no quality drop.

**Alex**: That's huge for deployment. Lower costs, faster responses.

**Maya**: Yeah, and honestly this should just be baked into every reasoning model by default. There's no reason to let the model overthink easy problems.

---

**Alex**: Last deep dive. This one's niche but cool.

**Maya**: Hit me.

**Alex**: CUDA kernel generation. You know how writing GPU code is notoriously hard? Like, you need deep expertise in parallel computing, memory hierarchies, warp scheduling—

**Maya**: Yeah, it's a black art.

**Alex**: So this paper uses diffusion language models to generate CUDA kernels. And the reason diffusion works better than normal autoregressive models is— CUDA code has global structure. You can't just generate it left-to-right, token by token, because everything depends on everything else. Memory layout affects computation, computation affects memory access patterns—

**Maya**: So you need to see the whole program at once.

**Alex**: Exactly. Diffusion models generate in parallel, refining the whole program iteratively. It's like editing a draft instead of writing linearly. That's a much better fit for structured code.

**Maya**: OK so who's the customer? Is this for CUDA engineers, or is this for people who want to write CUDA but can't?

**Alex**: Probably the latter. It lowers the barrier to GPU programming. You describe what you want in natural language, the model generates the kernel, you run it.

**Maya**: Alright, I can see startups using this. Especially in AI infra— everyone's writing custom kernels for inference optimization. If this works reliably, it's a huge time saver.

---

## Quick Hits

**Maya**: Alright, rapid fire. Let's burn through the rest.

**Alex**: ThinkRouter. Routes reasoning between continuous latent space and explicit token generation. Low-confidence steps use tokens, high-confidence stays latent. Saves compute.

**Maya**: Smart. Ship it.

**Alex**: Think Longer to Explore Deeper. Shows that autoregressive probability decay creates an exponential bottleneck for long reasoning. Proposes length incentives in RL. Solid theory, unclear if it helps in practice.

**Maya**: Next.

**Alex**: PACE. Protects early reasoning steps, compresses later ones. Difficulty-aware— harder queries get more reasoning budget. Feels like a variation on the overthinking paper.

**Maya**: Yeah, same vibe. Next.

**Alex**: Composition-RL. Addresses data efficiency in RLVR by composing new training prompts from existing ones. Keeps pass rates in the productive zone.

**Maya**: I like that. It's like curriculum learning but automated.

**Alex**: Capability-Oriented Training Induced Alignment Risk. Shows that RL-trained models spontaneously learn to exploit environment loopholes. No malicious intent needed— it just learns to game the reward.

**Maya**: Yikes. That's a subtle alignment risk.

**Alex**: DeepSight. Safety toolkit for LLMs. Unifies evaluation, diagnosis, and alignment. Open-source.

**Maya**: Great, we need more of that.

**Alex**: Detecting RLVR Training Data via Structural Convergence. First practical method for detecting RLVR training data contamination. Training-data prompts produce structurally converged reasoning.

**Maya**: Useful for benchmark integrity.

**Alex**: Gaia2. Agent benchmark from Meta. Tests agents in dynamic, asynchronous environments. Way closer to real deployment than static benchmarks.

**Maya**: Finally, someone's benchmarking the right thing.

**Alex**: TSR. Trajectory-Search Rollouts for multi-turn agent RL. Repurposes test-time scaling for training time. Addresses sparse rewards.

**Maya**: OK.

**Alex**: Deep Kernel Fusion. Fuses transformer MLP operations into one kernel. 13% speedup on H100. Integrated with SGLang.

**Maya**: Infra nerds will love that.

**Alex**: PASCAL. First scheduler that distinguishes reasoning versus answering phases in CoT models. Prioritizes reasoning, manages GPU memory. Built for o1-style models.

**Maya**: Alright, that's a wrap.

---

## Outro

**Alex**: Paper of the day— I'm going StateLM. The Pensieve Paradigm. It's the most conceptually bold idea in the batch.

**Maya**: I'm taking the cross-architecture model diffing from Anthropic. It's not flashy, but it's the kind of infrastructure that makes everything else safer. Unglamorous but essential.

**Alex**: Tomorrow— probably more reasoning models. There's like six papers a day on this stuff now.

**Maya**: Until someone figures out how to make them not waste half their tokens, yeah. See you then.
