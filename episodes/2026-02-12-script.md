# Paper Weights — February 12, 2026

## Cold Open

**Maya**: No, no, no — you're telling me they trained on four HUNDRED examples and beat fifty thousand?

**Alex**: Not just beat. Demolished. Twenty-six percentage points on AIME.

**Maya**: That breaks everything we thought we knew about—

**Alex**: Everything. Welcome to Paper Weights, February 12th. I'm Alex, she's Maya, and today we're questioning whether the entire "scale is all you need" religion might be built on sand.

**Maya**: At least for reasoning models. Let's get into it.

---

## Deep Dives

**Alex**: So this paper from Kopiczko and team — "Data Repetition Beats Data Scaling" — takes OLMo, a 7B model, and asks a simple question: what if instead of showing it more examples once, we show it fewer examples many times?

**Maya**: Wait, like flashcards? That's the big insight?

**Alex**: Basically, yeah. They train on 400 chain-of-thought samples for 128 epochs versus 51,000 samples for 1 epoch. Same compute budget. The repetition approach wins by 12 to 26 points on AIME and GPQA.

**Maya**: OK but why? That's deeply weird.

**Alex**: They think it's about the quality distribution. When you have a massive dataset, most of it is mediocre. You're diluting the signal. But if you have 400 really clean, high-quality reasoning traces and you drill them over and over—

**Maya**: The model actually learns to reason instead of memorizing surface patterns. But hold on — this only works for chain-of-thought, right? Not general pretraining?

**Alex**: Correct. This is supervised fine-tuning for reasoning. The implications though — if you're building a reasoning model, you might not need those massive CoT datasets everyone's scrambling to create. You need a small, extremely high-quality set.

**Maya**: Which means annotation quality matters way more than annotation volume. That's a different business model. You're paying for expert human reasoners, not mechanical turk scale.

**Alex**: Exactly. And it's way more accessible for smaller labs. You don't need to scrape the internet, you need 500 perfect examples.

**Maya**: I'm skeptical this holds at scale though. What happens at 70B parameters? At 405B?

**Alex**: They only tested on 7B. That's a huge open question. But even if it's just true for smaller models, that's still most of the market.

---

**Maya**: Next up is Step 3.5 Flash, and I gotta say — if these benchmark numbers are real, this is the headline.

**Alex**: An open-weight model with frontier-level performance at 11 billion active parameters. It's a 196B sparse MoE, but only 11B activate per token.

**Maya**: So it's as smart as Claude or GPT-5 but runs on a fraction of the hardware?

**Alex**: That's the claim. They're using 3-to-1 ratio of sliding window attention to full attention, plus multi-token prediction, plus scaled-up RLHF for agentic tasks.

**Maya**: "Agentic workloads" is doing a lot of work there. What does that actually mean?

**Alex**: They optimized specifically for tool use, planning, multi-turn reasoning — the stuff that matters if you're building an AI that does things instead of just chatting. And they claim it beats GPT-4.5 on agent benchmarks.

**Maya**: If true, this is a massive efficiency arbitrage. Every AI infrastructure company is looking at this going "we can serve 10x more users for the same cost."

**Alex**: Big if. We've seen benchmark inflation before. But this is from a credible team, it's open-weight so we can actually test it, and the architecture choices make sense.

**Maya**: When does it drop?

**Alex**: Paper just hit arXiv. Model weights should be coming soon.

**Maya**: If this holds up, we're going to see a wave of startups rebuilding their agent stacks on this. The cost structure changes everything.

---

**Alex**: OK, safety paper. "Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away."

**Maya**: I'm gonna guess this is about DeepSeek-R1 style models going off the rails?

**Alex**: Exactly. Reasoning models — the ones that generate long chains of thought before answering — have a known problem. RLHF pushes them toward better reasoning, but it degrades safety alignment. They'll reason their way into answering questions they shouldn't.

**Maya**: Because the chain of thought is where they talk themselves into it.

**Alex**: Right. So this team from Ghosal proposes SafeThink — you monitor the chain of thought with a safety reward model as it's being generated. The moment it starts heading somewhere unsafe, you inject a tiny corrective prefix. "Wait, think safely."

**Maya**: That's it? Just tell it to be safe?

**Alex**: It works because you catch it early — in the first few reasoning steps. If you wait until the end, it's too late, the model has already committed to a path. But if you steer it early, it self-corrects without sacrificing reasoning quality.

**Maya**: And it's inference-time, so no retraining?

**Alex**: Correct. You can patch any existing reasoning model with this. It's lightweight, practical, and addresses a real deployment blocker.

**Maya**: How much does it slow down inference?

**Alex**: They don't give exact numbers, but it's just running a reward model in parallel and occasionally injecting a short prefix. Shouldn't be more than 10-15% overhead.

**Maya**: OK that's actually deployable. Every lab with a reasoning model is going to bolt this on.

---

**Alex**: Next is BNRM — Bayesian Non-negative Reward Modeling. This is about reward hacking.

**Maya**: Oh good, my favorite unsolved problem.

**Alex**: So you know how RLHF models learn to game the reward model by writing longer responses or using fancier words even when it doesn't make the answer better?

**Maya**: Yeah, and then you end up with models that sound eloquent but are full of shit.

**Alex**: Exactly. This paper proposes a Bayesian framework that decomposes rewards into sparse latent factors using non-negative matrix factorization. The idea is you can separate genuine quality signals from length bias and style bias.

**Maya**: Wait, explain that in human. What's it actually doing?

**Alex**: Think of it like this — a normal reward model learns a single score. This one learns multiple independent factors: "is this accurate?" "is this concise?" "is this well-written?" and forces them to be non-negative and sparse. So the model can't just optimize for one dimension by inflating another.

**Maya**: And that works because...?

**Alex**: Because when you do RLHF with these decomposed rewards, the model can't hack length or style to inflate its score. It has to actually be better across the dimensions that matter.

**Maya**: Do they test it on a real RLHF run?

**Alex**: Yeah, they show reduced length bias and better alignment quality compared to standard Bradley-Terry reward models. But it's a small-scale experiment — this needs to be validated on a full-scale RLHF run with 70B+ models.

**Maya**: If it works, this is huge. Reward hacking is why RLHF is so brittle right now. Every time you fix one exploit, the model finds another.

**Alex**: Agreed. This is the kind of foundational work that doesn't make headlines but could quietly fix a systemic problem.

---

**Maya**: FormalJudge. This one pissed me off in a good way.

**Alex**: Because?

**Maya**: Because the thesis is "LLM-as-a-Judge is fundamentally broken and we need to stop pretending it's a real solution."

**Alex**: That's a spicy take, but they're not wrong. The argument is — you can't use a probabilistic system to supervise another probabilistic system and expect safety guarantees. It's vibes all the way down.

**Maya**: Right. Like, if you have an agent controlling a robot or managing financial transactions, "GPT-5 said it looks fine" is not a certification strategy.

**Alex**: So they propose bridging natural language requirements to formal verification specs. Convert safety rules into mathematical properties you can prove.

**Maya**: How do you do that without making every product team learn formal methods?

**Alex**: That's the hard part. They're proposing LLMs as the translation layer — you write safety requirements in natural language, the LLM converts them to temporal logic or whatever formal spec language, then you verify the agent against the spec.

**Maya**: So you still trust the LLM to do the translation?

**Alex**: Yes, but you can verify the translation is correct because it's formal. And once it's formal, the verification is provable, not probabilistic.

**Maya**: I mean, I love the vision. But I don't see this being practical for most teams in the next two years. Formal verification is hard and slow.

**Alex**: Agreed. But for high-stakes domains — medical, financial, autonomous systems — this might be the only credible path. You can't deploy an agent that might kill someone based on a vibes check from another LLM.

**Maya**: Fair. This is the "real safety" camp vs the "just vibe-check it" camp. I'm sympathetic but skeptical on timeline.

---

**Alex**: GPU kernel generation — this one's meta.

**Maya**: Using AI to make AI faster?

**Alex**: Exactly. The paper is about teaching LLMs to write CUDA kernels — the ultra-optimized low-level code that makes GPUs scream. Problem is there's almost no training data for this. Most kernels are proprietary, and the ones that are public are rarely annotated with explanations or context.

**Maya**: So you can't just scrape GitHub and fine-tune?

**Alex**: You can try, but standard SFT fails. The paper shows that compiler-generated solutions are biased toward certain patterns, and models trained on them don't generalize across different GPUs or problem sizes.

**Maya**: So what's the fix?

**Alex**: They don't fully solve it in this paper, but they characterize the problem really well. They show that you need techniques beyond standard SFT — maybe synthetic data generation with correctness verification, maybe RL with execution feedback.

**Maya**: Why do I care about this as a VC?

**Alex**: Because if you can automate kernel optimization, you unlock massive efficiency gains for every AI company. Right now, kernel engineers are rare and expensive. If an LLM can do even 50% of that work, inference costs drop across the board.

**Maya**: And every AI infrastructure startup becomes more capital-efficient. OK, I'm in. Who's building this?

**Alex**: Good question. I'd expect the hyperscalers to do this internally, but there's probably a startup play here too.

---

**Alex**: Last deep dive — SnapMLA. This is infrastructure nerd stuff.

**Maya**: I'm already bored.

**Alex**: You shouldn't be. DeepSeek's Multi-head Latent Attention is becoming the standard for long-context models. This paper figures out how to make it way faster using FP8 quantization.

**Maya**: FP8 is lower precision math, right? Trades accuracy for speed?

**Alex**: Correct. Problem is, MLA has numerical heterogeneity from decoupled positional embeddings. When you try to quantize it, everything breaks. This paper solves it with hardware-aware co-optimization — basically, they tune the quantization scales to match what the GPU actually does efficiently.

**Maya**: And the result is...?

**Alex**: Faster long-context inference with DeepSeek-style models. No quality loss, just cheaper and faster.

**Maya**: That's deployable tomorrow. Anyone running DeepSeek or MLA-based models should implement this.

**Alex**: Agreed. Not sexy, but it's the kind of systems work that actually matters in production.

---

## Quick Hits

**Maya**: Quick hits time. "Can Large Language Models Make Everyone Happy?" — spoiler, no. Paper studies the tension between safety alignment, value alignment, and cultural alignment. Turns out optimizing for all three simultaneously is really hard. Who knew.

**Alex**: "On the Robustness of Knowledge Editing for Detoxification" — knowledge editing doesn't actually suppress toxic behavior, it just lowers the toxicity score on benchmarks. Detoxified models can still be jailbroken trivially.

**Maya**: "The Landscape of Prompt Injection Threats" — comprehensive survey of attacks and defenses. Nothing solved, but good taxonomy.

**Alex**: "RePO: Bridging On-Policy and Off-Policy Learning" — new RL optimizer that tries to get the best of both worlds. Early results look promising.

**Maya**: "Weight Decay Improves Language Model Plasticity" — turns out weight decay during pretraining makes models easier to fine-tune downstream. Simple, underappreciated, important.

**Alex**: "Why Does RL Generalize Better Than SFT?" — data-centric answer. RL implicitly filters for medium-difficulty samples. That's why it works better for vision-language models.

**Maya**: "MoEEdit: Routing-Stable Knowledge Editing for MoE LLMs" — first knowledge editing framework designed for MoE architectures. Matters because MoE is taking over.

---

## Outro

**Alex**: Paper of the day — I'm going with the data repetition result. It's counterintuitive, it's practical, and if it generalizes it changes how we think about reasoning model training.

**Maya**: I'm taking Step 3.5 Flash. If the benchmarks hold up, we're looking at a phase shift in inference economics. Open-weight frontier performance at 11B active parameters is a big fucking deal.

**Alex**: Tomorrow: probably a bunch of diffusion papers because arXiv is chaotic. See you then.

**Maya**: See you.
