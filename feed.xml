<?xml version="1.0" ?>
<rss xmlns:ns0="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Paper Weights: Daily AI Research Briefing</title>
    <description>Every morning, two hosts break down the AI papers that actually matter — one explains the science, one asks where the money is. Hundreds of papers filtered down to the dozen or so that could become products, disrupt markets, or change how you build. 15 minutes. No filler.</description>
    <link>https://github.com/antonber/paper-weights-podcast</link>
    <language>en</language>
    <ns0:author>Paper Weights</ns0:author>
    <ns0:explicit>no</ns0:explicit>
    <ns0:category text="Technology"/>
    <ns0:image href="https://github.com/antonber/paper-weights-podcast/releases/download/assets/cover-art.png"/>
    <item>
      <title>BFS-PO: Best-First Search for Large Reasoning Models</title>
      <description>Today's episode dives deep into BFS-PO: Best-First Search for Large Reasoning Models, along with Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models, Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens. Plus 10 quick hits covering the rest of what dropped on arXiv. Alex breaks down the technical details while Maya asks the hard questions about what actually matters for building products and making money. 17 papers, zero filler.

Deep Dives:
• [00:39] BFS-PO: Best-First Search for Large Reasoning Models — https://arxiv.org/abs/2602.14917
• [01:49] Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models — https://arxiv.org/abs/2602.14917
• [03:00] Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens — https://arxiv.org/abs/2602.13517
• [04:42] AMOR: Entropy-Based Metacognitive Gate for Dynamic SSM-Attention Switching — https://arxiv.org/abs/2602.13215
• [06:00] Open Rubric System: Scaling RL with Pairwise Adaptive Rubrics — https://arxiv.org/abs/2602.14069
• [07:50] OneLatent: Single-Token Compression for Visual Latent Reasoning — https://arxiv.org/abs/2602.13738
• [10:04] Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of MCP — https://arxiv.org/abs/2602.13320

Quick Hits:
• [11:38] Speculative Decoding with a Speculative Vocabulary — https://arxiv.org/abs/2602.13836
• GradMAP: Faster Layer Pruning with Gradient Metric and Projection Compensation — https://arxiv.org/abs/2602.14649
• AllMem: A Memory-centric Recipe for Efficient Long-context Modeling — https://arxiv.org/abs/2602.13680
• Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment — https://arxiv.org/abs/2602.13575
• Small Reward Models via Backward Inference — https://arxiv.org/abs/2602.13551
• Measuring and Mitigating Post-hoc Rationalization in Reverse Chain-of-Thought — https://arxiv.org/abs/2602.14469
• REMem: Reasoning with Episodic Memory in Language Agents — https://arxiv.org/abs/2602.13530
• GUI-GENESIS: Automated Synthesis of Environments with Verifiable Rewards for GUI Agent Post-Training — https://arxiv.org/abs/2602.14093
• REDSearcher: A Scalable Framework for Long-Horizon Search Agents — https://arxiv.org/abs/2602.14234
• LogitsCoder: Efficient Chain-of-Thought Path Search via Logits Preference Decoding — https://arxiv.org/abs/2602.14054</description>
      <link>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-17/2026-02-17-podcast.mp3</link>
      <guid>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-17/2026-02-17-podcast.mp3</guid>
      <pubDate>Tue, 17 Feb 2026 09:00:00 -0600</pubDate>
      <enclosure url="https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-17/2026-02-17-podcast.mp3" length="12519063" type="audio/mpeg"/>
      <ns0:duration>00:13:35</ns0:duration>
    </item>
    <item>
      <title>Decoupling Reasoning Proposals from Decisions</title>
      <description>Today's episode dives deep into Decoupling Reasoning Proposals from Decisions, along with Cognitive Budget Allocation for Agents, The Reasoning Model Attack Surface (EXTENDED). Alex breaks down the technical details while Maya asks the hard questions about what actually matters for building products and making money. 7 papers, zero filler.

Papers discussed:
• [00:51] Decoupling Reasoning Proposals from Decisions — https://arxiv.org/abs/2602.12846v1
• [02:18] Cognitive Budget Allocation for Agents — https://arxiv.org/abs/2602.12662v1
• [03:44] The Reasoning Model Attack Surface (EXTENDED)
• [06:20] Verifier-Free Reasoning Training
• [08:04] Shorter Chains of Thought Without the Accuracy Hit
• [09:39] The Diversity Illusion in Self-Play Training — https://arxiv.org/abs/2602.13103v1
• [11:14] Web Agent Training at Scale — https://arxiv.org/abs/2602.12544v1</description>
      <link>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-16/2026-02-16-podcast.mp3</link>
      <guid>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-16/2026-02-16-podcast.mp3</guid>
      <pubDate>Mon, 16 Feb 2026 09:00:00 -0600</pubDate>
      <enclosure url="https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-16/2026-02-16-podcast.mp3" length="14115715" type="audio/mpeg"/>
      <ns0:duration>00:15:16</ns0:duration>
    </item>
    <item>
      <title>Pensieve Paradigm</title>
      <description>Today we're breaking down Pensieve Paradigm. Alex explains the science, Maya asks where the money is. 1 papers that could change how you build.

Papers discussed:
• [00:56] Pensieve Paradigm — https://arxiv.org/abs/2602.12108</description>
      <link>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-14/2026-02-14-podcast.mp3</link>
      <guid>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-14/2026-02-14-podcast.mp3</guid>
      <pubDate>Sat, 14 Feb 2026 09:00:00 -0600</pubDate>
      <enclosure url="https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-14/2026-02-14-podcast.mp3" length="8497174" type="audio/mpeg"/>
      <ns0:duration>00:09:10</ns0:duration>
    </item>
    <item>
      <title>Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of ...</title>
      <description>Today's episode dives deep into Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs, along with Native Reasoning Models: Training Language Models to Reason on Unverifiable Data, MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling. Plus 11 quick hits covering the rest of what dropped on arXiv. Alex breaks down the technical details while Maya asks the hard questions about what actually matters for building products and making money. 18 papers, zero filler.

Deep Dives:
• [00:47] Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs — https://arxiv.org/abs/2602.11729v1
• [01:50] Native Reasoning Models: Training Language Models to Reason on Unverifiable Data — https://arxiv.org/abs/2602.11549v1
• [03:33] MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling — https://arxiv.org/abs/2602.11761v1
• [05:00] Extending Puzzle for Mixture-of-Experts Reasoning Models (GPT-OSS Acceleration) — https://arxiv.org/abs/2602.11549v1
• [06:35] The Pensieve Paradigm: StateLM — Stateful Language Models Mastering Their Own Context — https://arxiv.org/abs/2602.11549v1
• [08:33] Stop Unnecessary Reflection: Adaptive Reflection and Length Coordinated Penalty for LRMs — https://arxiv.org/abs/2602.12113v1
• [10:00] DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels — https://arxiv.org/abs/2602.11549v1

Quick Hits:
• [11:19] ThinkRouter: Efficient Reasoning via Routing Between Latent and Discrete Spaces — https://arxiv.org/abs/2602.11683v1
• Think Longer to Explore Deeper: In-Context Exploration via Length-Incentivized RL — https://arxiv.org/abs/2602.11748v1
• PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning — https://arxiv.org/abs/2602.11683v1
• Composition-RL: Composing Verifiable Prompts for RL of LLMs — https://arxiv.org/abs/2602.12036v1
• Capability-Oriented Training Induced Alignment Risk — https://arxiv.org/abs/2602.12124v1
• DeepSight: An All-in-One LM Safety Toolkit — https://arxiv.org/abs/2602.12092v1
• Detecting RLVR Training Data via Structural Convergence of Reasoning — https://arxiv.org/abs/2602.11549v1
• Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments — https://arxiv.org/abs/2602.11964v1
• TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents — https://arxiv.org/abs/2602.11767v1
• Deep Kernel Fusion for Transformers — https://arxiv.org/abs/2602.11808v1
• PASCAL: Phase-Aware Scheduling for Serving Reasoning-based LLMs — https://arxiv.org/abs/2602.11530v1</description>
      <link>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-13/2026-02-13-podcast.mp3</link>
      <guid>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-13/2026-02-13-podcast.mp3</guid>
      <pubDate>Fri, 13 Feb 2026 09:00:00 -0600</pubDate>
      <enclosure url="https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-13/2026-02-13-podcast.mp3" length="13708463" type="audio/mpeg"/>
      <ns0:duration>00:14:53</ns0:duration>
    </item>
    <item>
      <title>Data Repetition Beats Data Scaling</title>
      <description>Today's episode dives deep into Data Repetition Beats Data Scaling, along with Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away.. Plus 7 quick hits covering the rest of what dropped on arXiv. Alex breaks down the technical details while Maya asks the hard questions about what actually matters for building products and making money. 9 papers, zero filler.

Deep Dives:
• [00:42] Data Repetition Beats Data Scaling — http://arxiv.org/abs/2602.07839v1
• [02:15] Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away. — http://arxiv.org/abs/2602.07831v1

Quick Hits:
• [12:02] Can Large Language Models Make Everyone Happy?
• On the Robustness of Knowledge Editing for Detoxification
• The Landscape of Prompt Injection Threats
• RePO: Bridging On-Policy and Off-Policy Learning
• Weight Decay Improves Language Model Plasticity
• Why Does RL Generalize Better Than SFT?
• MoEEdit: Routing-Stable Knowledge Editing for MoE LLMs</description>
      <link>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-12/2026-02-12-podcast.mp3</link>
      <guid>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-12/2026-02-12-podcast.mp3</guid>
      <pubDate>Thu, 12 Feb 2026 09:00:00 -0600</pubDate>
      <enclosure url="https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-12/2026-02-12-podcast.mp3" length="12560905" type="audio/mpeg"/>
      <ns0:duration>00:13:35</ns0:duration>
    </item>
    <item>
      <title>The Geometry of Thinking</title>
      <description>Today's episode dives deep into The Geometry of Thinking, along with Reading the Model's Mind Before It Speaks, The Safety Trilemma. Plus 1 quick hits covering the rest of what dropped on arXiv. Alex breaks down the technical details while Maya asks the hard questions about what actually matters for building products and making money. 7 papers, zero filler.

Deep Dives:
• [00:40] The Geometry of Thinking
• [01:54] Reading the Model's Mind Before It Speaks
• [03:26] The Safety Trilemma
• [05:09] Finding the Point of No Return
• [06:29] The Token Efficiency Problem
• [08:47] We Finally Know Why Reasoning Works

Quick Hits:
• [10:53] Knowledge Integration Decay — https://arxiv.org/abs/2602.09517</description>
      <link>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-11/2026-02-11-podcast-v2.mp3</link>
      <guid>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-11/2026-02-11-podcast-v2.mp3</guid>
      <pubDate>Wed, 11 Feb 2026 09:00:00 -0600</pubDate>
      <enclosure url="https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-11/2026-02-11-podcast-v2.mp3" length="11426453" type="audio/mpeg"/>
      <ns0:duration>00:12:36</ns0:duration>
    </item>
    <item>
      <title>iGRPO — Self-Feedback-Driven LLM Reasoning</title>
      <description>Today's episode dives deep into iGRPO — Self-Feedback-Driven LLM Reasoning, along with Next Concept Prediction — Beyond Token-Level Thinking, CoRefine — Confidence-Guided Self-Refinement. Alex breaks down the technical details while Maya asks the hard questions about what actually matters for building products and making money. 7 papers, zero filler.

Papers discussed:
• [00:36] iGRPO — Self-Feedback-Driven LLM Reasoning — http://arxiv.org/abs/2602.09000v1
• [01:39] Next Concept Prediction — Beyond Token-Level Thinking — http://arxiv.org/abs/2602.08984v1
• [02:51] CoRefine — Confidence-Guided Self-Refinement — http://arxiv.org/abs/2602.08948v1
• [04:03] DirMoE — Dirichlet-Routed Mixture of Experts — http://arxiv.org/abs/2602.09001v1
• [05:15] InternAgent — The Robot Scientist
• [06:54] Misaligned Actions in Computer-Use Agents — http://arxiv.org/abs/2602.08995v1
• [08:24] Goal-Directedness in Language Model Agents — http://arxiv.org/abs/2602.08964v1</description>
      <link>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-10/2026-02-10-podcast.mp3</link>
      <guid>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-10/2026-02-10-podcast.mp3</guid>
      <pubDate>Tue, 10 Feb 2026 09:00:00 -0600</pubDate>
      <enclosure url="https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-10/2026-02-10-podcast.mp3" length="13906181" type="audio/mpeg"/>
      <ns0:duration>00:15:01</ns0:duration>
    </item>
    <item>
      <title>InftyThink+ (Reasoning Efficiency)</title>
      <description>Today's episode dives deep into InftyThink+ (Reasoning Efficiency), along with Generative Meta-Model of LLM Activations (Interpretability), DAWN (Diffusion LLM Speedup). Alex breaks down the technical details while Maya asks the hard questions about what actually matters for building products and making money. 7 papers, zero filler.

Papers discussed:
• [00:00] InftyThink+ (Reasoning Efficiency)
• [00:43] Generative Meta-Model of LLM Activations (Interpretability) — https://arxiv.org/abs/2602.06964
• [02:52] DAWN (Diffusion LLM Speedup)
• [04:40] Agentic Overconfidence — https://arxiv.org/abs/2602.06948
• [06:06] Endogenous Steering Resistance — https://arxiv.org/abs/2602.06941
• [07:54] Multi-Objective Alignment Interference — https://arxiv.org/abs/2602.06869
• [09:20] Halluverse Multilingual Hallucination Benchmark — https://arxiv.org/abs/2602.06920</description>
      <link>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-09/2026-02-09-podcast-v2.mp3</link>
      <guid>https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-09/2026-02-09-podcast-v2.mp3</guid>
      <pubDate>Mon, 09 Feb 2026 09:00:00 -0600</pubDate>
      <enclosure url="https://github.com/antonber/paper-weights-podcast/releases/download/2026-02-09/2026-02-09-podcast-v2.mp3" length="10234614" type="audio/mpeg"/>
      <ns0:duration>00:12:56</ns0:duration>
    </item>
  </channel>
</rss>
